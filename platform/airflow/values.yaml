# Airflow Helm Chart Values
# Default configuration for Apache Airflow workflow orchestrator
# Override with environment-specific values files

# Global settings
global:
  namespace: lakehouse-platform

# Airflow subchart configuration
# All values are passed to the upstream Airflow Helm chart
airflow:
  # Executor type: KubernetesExecutor for container-based jobs
  # Reference: contracts/workflow-orchestration.md section 5.1
  executor: "KubernetesExecutor"
  
  # Secrets
  fernetKeySecretName: airflow-fernet-key
  webserverSecretKeySecretName: airflow-webserver-secret

  # Image configuration
  images:
    airflow:
      repository: apache/airflow
      tag: "2.8.0-python3.11"
      pullPolicy: IfNotPresent

  # Use existing secret for admin password
  # The secret should have key: admin-password
  adminUserSecretName: "airflow-admin-password"

  # Webserver configuration
  webserver:
    replicas: 1

    resources:
      requests:
        memory: 512Mi
        cpu: 250m
      limits:
        memory: 1Gi
        cpu: 500m

    service:
      type: ClusterIP

    # Default user credentials
    # Note: Password is set from secret 'airflow-admin-password'
    # After first login, change password through the UI
    defaultUser:
      enabled: false
      role: Admin
      username: admin
      email: admin@example.com
      firstName: Admin
      lastName: User
      password: ""  # Set from secret

    # Probes configuration
    livenessProbe:
      initialDelaySeconds: 60
      timeoutSeconds: 30
      failureThreshold: 5
      periodSeconds: 20

    readinessProbe:
      initialDelaySeconds: 60
      timeoutSeconds: 30
      failureThreshold: 5
      periodSeconds: 20

    startupProbe:
      timeoutSeconds: 30
      failureThreshold: 60
      periodSeconds: 10

  # Scheduler configuration
  scheduler:
    replicas: 1
    
    resources:
      requests:
        memory: 512Mi
        cpu: 250m
      limits:
        memory: 1Gi
        cpu: 500m

  # PostgreSQL (metadata database)
  postgresql:
    enabled: true
    image:
      registry: docker.io
      repository: bitnami/postgresql
      tag: "latest"
    auth:
      existingSecret: postgres-creds
      database: airflow
    primary:
      extraEnvVars:
      extraEnvVars:
        - name: POSTGRESQL_USERNAME
          valueFrom:
            secretKeyRef:
              name: postgres-creds
              key: username
        - name: POSTGRESQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-creds
              key: password
      persistence:
        enabled: true
        size: 8Gi
      initdb:
        scripts:
          create-iceberg-db.sql: |
            CREATE DATABASE iceberg;

  # Redis (disabled for KubernetesExecutor)
  redis:
    enabled: false

  # DAGs configuration
  dags:
    persistence:
      enabled: true
      size: 1Gi
      storageClassName: ""
    
    # GitSync configuration for DAG deployment
    gitSync:
      enabled: false
      repo: ""
      branch: main
      subPath: "workflows/"
      wait: 60
      maxFailures: 0

  # Airflow configuration
  # Reference: contracts/workflow-orchestration.md section 12
  config:
    core:
      dags_folder: "/opt/airflow/dags"
      load_examples: "False"
      executor: "KubernetesExecutor"
      parallelism: 32
      max_active_tasks_per_dag: 16
      max_active_runs_per_dag: 16
    
    webserver:
      expose_config: "True"
      rbac: "True"
      authenticate: "True"
      auth_backend: "airflow.api.auth.backend.basic_auth"
      enable_proxy_fix: "True"
    
    api:
      auth_backends: "airflow.api.auth.backend.basic_auth"
    
    kubernetes:
      namespace: lakehouse-platform
      worker_container_repository: apache/airflow
      worker_container_tag: "2.8.0-python3.11"
      delete_worker_pods: "True"
      delete_worker_pods_on_failure: "False"
      worker_pods_creation_batch_size: 1
      multi_namespace_mode: "False"
    
    logging:
      remote_logging: "False"
      colored_console_log: "True"
      logging_level: "INFO"
    
    metrics:
      statsd_on: "False"

  # Environment variables
  env:
    - name: AIRFLOW__CORE__EXECUTOR
      value: "KubernetesExecutor"
    - name: AIRFLOW__KUBERNETES__NAMESPACE
      value: "lakehouse-platform"

  # Use existing secret for metadata connection
  data:
    metadataSecretName: airflow-db-connection

  # Extra environment variables
  extraEnv: null

  # Extra environment variables from secrets
  extraEnvFrom: null

  # Database migration job configuration
  migrateDatabaseJob:
    enabled: true
    useHelmHooks: false
    applyCustomEnv: true

  # Ingress configuration
  ingress:
    enabled: false
    web:
      enabled: false
      annotations: {}
      hosts: []

  # Metrics and monitoring
  metrics:
    enabled: false
    serviceMonitor:
      enabled: false
      interval: 30s
      selector:
        prometheus: kube-prometheus

  # Service account
  serviceAccount:
    create: true
    name: ""
    annotations: {}

  # Security context
  securityContext:
    runAsUser: 50000
    fsGroup: 0

  # Pod security context
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 50000
    fsGroup: 0

  # Extra volumes
  extraVolumes: []

  # Extra volume mounts
  extraVolumeMounts: []

  # Triggerer (for deferrable operators)
  triggerer:
    enabled: false
    replicas: 1
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 200m

