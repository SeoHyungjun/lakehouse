# Airflow Helm Chart Values
# Default configuration for Apache Airflow workflow orchestrator
# Override with environment-specific values files

# Global settings
global:
  namespace: lakehouse-platform

# Airflow subchart configuration
# All values are passed to the upstream Airflow Helm chart
airflow:
  # Executor type: KubernetesExecutor for container-based jobs
  # Reference: contracts/workflow-orchestration.md section 5.1
  executor: "KubernetesExecutor"
  
  # Secrets
  fernetKeySecretName: airflow-fernet-key
  webserverSecretKeySecretName: airflow-webserver-secret

  # Image configuration
  images:
    airflow:
      repository: apache/airflow
      tag: "3.0.2-python3.11"
      pullPolicy: IfNotPresent

  # Use existing secret for admin user credentials
  # The secret should have keys: admin-user, admin-password, admin-email, admin-firstname, admin-lastname
  # Reference: platform/secrets/admin-password-sealed-secret.yaml
  # Generated by: scripts/generate-secrets.sh
  adminUserSecretName: "airflow-admin-password"

  # Webserver configuration
  webserver:
    replicas: 1

    defaultUser:
      enabled: false

    resources:
      requests:
        memory: 512Mi
        cpu: 250m
      limits:
        memory: 1Gi
        cpu: 500m

    service:
      type: ClusterIP

    # Probes configuration
    livenessProbe:
      initialDelaySeconds: 60
      timeoutSeconds: 30
      failureThreshold: 5
      periodSeconds: 20

    readinessProbe:
      initialDelaySeconds: 60
      timeoutSeconds: 30
      failureThreshold: 5
      periodSeconds: 20

    startupProbe:
      timeoutSeconds: 30
      failureThreshold: 60
      periodSeconds: 10

  # Scheduler configuration
  scheduler:
    replicas: 1

    # Disable log groomer since remote_logging is enabled
    # Logs are stored in S3/MinIO, so local log cleanup is not needed
    logGroomerSidecar:
      enabled: false

    resources:
      requests:
        memory: 512Mi
        cpu: 250m
      limits:
        memory: 1Gi
        cpu: 500m

  # PostgreSQL (metadata database)
  postgresql:
    enabled: true
    image:
      registry: docker.io
      repository: bitnami/postgresql
      tag: "latest"
    auth:
      existingSecret: postgres-creds
      database: airflow
    primary:
      extraEnvVars:
        - name: POSTGRESQL_USERNAME
          valueFrom:
            secretKeyRef:
              name: postgres-creds
              key: username
        - name: POSTGRESQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-creds
              key: password
      persistence:
        enabled: true
        size: 8Gi
      initdb:
        scripts:
          create-iceberg-db.sql: |
            CREATE DATABASE iceberg;
          create-sales-data-table.sql: |
            CREATE TABLE IF NOT EXISTS sales_data (
                id SERIAL PRIMARY KEY,
                product_name VARCHAR(100),
                quantity INTEGER,
                price DECIMAL(10,2),
                sale_date DATE
            );
            INSERT INTO sales_data (product_name, quantity, price, sale_date) VALUES
                ('Laptop', 2, 999.99, '2026-01-10'),
                ('Phone', 5, 599.99, '2026-01-11'),
                ('Headphones', 10, 79.99, '2026-01-12'),
                ('Keyboard', 3, 149.99, '2026-01-13'),
                ('Mouse', 8, 49.99, '2026-01-14');

  # Redis (disabled for KubernetesExecutor)
  redis:
    enabled: false

  # DAGs configuration
  dags:
    persistence:
      enabled: false
      # size: 1Gi
      # storageClassName: ""

    # GitSync configuration for DAG deployment
    # gitSync:
    #   enabled: true
    #   repo: https://github.com/SeoHyungjun/lakehouse-dags.git
    #   branch: main
    #   subPath: "dags"
    #   wait: 60
    #   maxFailures: 0
    #   credentialsSecret: git-sync-cred

  # Airflow configuration
  # Reference: contracts/workflow-orchestration.md section 12
  config:
    core:
      # dags_folder: "/opt/airflow/dags"
      load_examples: "False"
      load_connections: "True"
      executor: "KubernetesExecutor"
      parallelism: 32
      max_active_tasks_per_dag: 16
      max_active_runs_per_dag: 16
      dag_dir_list_interval: 30
      min_file_process_interval: 5
    
    webserver:
      expose_config: "True"
      rbac: "True"
      authenticate: "True"
      auth_backend: "airflow.api.auth.backend.basic_auth"
      enable_proxy_fix: "True"
    
    api:
      auth_backends: "airflow.api.auth.backend.basic_auth"
    
    kubernetes:
      namespace: lakehouse-platform
      worker_container_repository: apache/airflow
      worker_container_tag: "3.0.2-python3.11"
      delete_worker_pods: "True"
      delete_worker_pods_on_failure: "False"
      worker_pods_creation_batch_size: 1
      multi_namespace_mode: "False"
    
    logging:
      remote_logging: "True"
      colored_console_log: "True"
      logging_level: "INFO"
      remote_base_log_folder: "s3://lakehouse-dev-warehouse/airflow-logs/"
      remote_logging_conn_id: "aws_default"
      remote_log_conn_id: "aws_default"
    
    metrics:
      statsd_on: "False"

  # Environment variables
  env:
    - name: AIRFLOW__CORE__EXECUTOR
      value: "KubernetesExecutor"
    - name: AIRFLOW__KUBERNETES__NAMESPACE
      value: "lakehouse-platform"

  # Use existing secret for metadata connection
  data:
    metadataSecretName: airflow-db-connection

  # Extra environment variables
  extraEnv: null

  # Environment variables from secrets for DAGs
  # Reference: https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/operators.html
  secret:
    - envName: AIRFLOW_CONN_AWS_DEFAULT
      secretName: airflow-connections
      secretKey: AIRFLOW_CONN_AWS_DEFAULT
    - envName: AWS_ACCESS_KEY_ID
      secretName: minio-creds
      secretKey: accessKeyId
    - envName: AWS_SECRET_ACCESS_KEY
      secretName: minio-creds
      secretKey: secretAccessKey
    - envName: MINIO_ACCESS_KEY
      secretName: minio-creds
      secretKey: accessKeyId
    - envName: MINIO_SECRET_KEY
      secretName: minio-creds
      secretKey: secretAccessKey
    - envName: MINIO_ENDPOINT_URL
      secretName: minio-creds
      secretKey: MINIO_ENDPOINT_URL
    # DAG에서 사용하는 AIRFLOW_ 접두사 환경변수
    - envName: AIRFLOW_MINIO_ENDPOINT_URL
      secretName: minio-creds
      secretKey: MINIO_ENDPOINT_URL
    - envName: AIRFLOW_MINIO_ACCESS_KEY
      secretName: minio-creds
      secretKey: accessKeyId
    - envName: AIRFLOW_MINIO_SECRET_KEY
      secretName: minio-creds
      secretKey: secretAccessKey
    - envName: AIRFLOW_MINIO_BUCKET
      secretName: minio-creds
      secretKey: MINIO_BUCKET
    # DAG에서 사용하는 AIRFLOW_ 접두사 환경변수
    - envName: AIRFLOW_TRINO_ENDPOINT_URL
      secretName: airflow-secrets
      secretKey: AIRFLOW_TRINO_ENDPOINT_URL
    - envName: AIRFLOW_TRINO_CATALOG
      secretName: airflow-secrets
      secretKey: AIRFLOW_TRINO_CATALOG
    - envName: AIRFLOW_TRINO_SCHEMA
      secretName: airflow-secrets
      secretKey: AIRFLOW_TRINO_SCHEMA
    - envName: AIRFLOW_POSTGRES_HOST
      secretName: airflow-secrets
      secretKey: AIRFLOW_POSTGRES_HOST
    - envName: AIRFLOW_POSTGRES_PORT
      secretName: airflow-secrets
      secretKey: AIRFLOW_POSTGRES_PORT
    - envName: AIRFLOW_POSTGRES_DATABASE
      secretName: airflow-secrets
      secretKey: AIRFLOW_POSTGRES_DATABASE
    - envName: AIRFLOW_POSTGRES_USERNAME
      secretName: airflow-secrets
      secretKey: AIRFLOW_POSTGRES_USERNAME
    - envName: AIRFLOW_POSTGRES_PASSWORD
      secretName: airflow-secrets
      secretKey: AIRFLOW_POSTGRES_PASSWORD
    - envName: AIRFLOW_CONN_POSTGRES_DEFAULT
      secretName: airflow-connections
      secretKey: AIRFLOW_CONN_POSTGRES_DEFAULT
    - envName: KDP_REQUEST_URL
      secretName: airflow-secrets
      secretKey: KDP_REQUEST_URL
    - envName: KDP_REGION
      secretName: airflow-secrets
      secretKey: KDP_REGION
    - envName: KDP_OPT_TYPE
      secretName: airflow-secrets
      secretKey: KDP_OPT_TYPE
    - envName: KDP_TOKEN_URL
      secretName: airflow-secrets
      secretKey: KDP_TOKEN_URL
    - envName: KDP_OAUTH_USERNAME
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_USERNAME
    - envName: KDP_OAUTH_CLIENT_ID_PROD
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_CLIENT_ID_PROD
    - envName: KDP_OAUTH_CLIENT_SECRET_PROD
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_CLIENT_SECRET_PROD
    - envName: KDP_OAUTH_PASSWORD_PROD
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_PASSWORD_PROD
    - envName: KDP_OAUTH_CLIENT_ID_DEV
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_CLIENT_ID_DEV
    - envName: KDP_OAUTH_CLIENT_SECRET_DEV
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_CLIENT_SECRET_DEV
    - envName: KDP_OAUTH_PASSWORD_DEV
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_PASSWORD_DEV
    - envName: KDP_OAUTH_USERNAME_USER2
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_USERNAME_USER2
    - envName: KDP_OAUTH_CLIENT_ID_USER2_PROD
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_CLIENT_ID_USER2_PROD
    - envName: KDP_OAUTH_CLIENT_SECRET_USER2_PROD
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_CLIENT_SECRET_USER2_PROD
    - envName: KDP_OAUTH_PASSWORD_USER2_PROD
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_PASSWORD_USER2_PROD
    - envName: KDP_OAUTH_CLIENT_ID_USER2_DEV
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_CLIENT_ID_USER2_DEV
    - envName: KDP_OAUTH_CLIENT_SECRET_USER2_DEV
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_CLIENT_SECRET_USER2_DEV
    - envName: KDP_OAUTH_PASSWORD_USER2_DEV
      secretName: airflow-secrets
      secretKey: KDP_OAUTH_PASSWORD_USER2_DEV
    - envName: AZURE_CONNECTION_STRING
      secretName: airflow-secrets
      secretKey: AZURE_CONNECTION_STRING
    - envName: AZURE_CONTAINER_NAME
      secretName: airflow-secrets
      secretKey: AZURE_CONTAINER_NAME

  # Extra environment variables from secrets
  # Note: extraEnvFrom expects a templated string, not an array
  extraEnvFrom: |
    - secretRef:
        name: airflow-connections
    - secretRef:
        name: minio-creds
    - secretRef:
        name: git-sync-cred
    - secretRef:
        name: airflow-secrets

  # Database migration job configuration
  # Reference: https://github.com/apache/airflow/tree/main/chart
  # For ArgoCD/Flux/Rancher/Terraform, useHelmHooks and applyCustomEnv MUST be false
  migrateDatabaseJob:
    enabled: true
    useHelmHooks: false
    applyCustomEnv: false
    # Automatically run migrations on ArgoCD sync
    jobAnnotations:
      "argocd.argoproj.io/hook": Sync
    # Add connection initialization script
    command:
      - /bin/sh
      - -c
      - |
        airflow db migrate && \
        python3 -c "
        import os
        import json
        from airflow.models import Connection
        from airflow import settings

        session = settings.Session()

        # Get MinIO endpoint from environment variable (from minio-creds secret)
        minio_endpoint = os.environ.get('MINIO_ENDPOINT_URL', 'http://minio:9000')
        minio_access_key = os.environ.get('MINIO_ACCESS_KEY', 'lakehouse')
        minio_secret_key = os.environ.get('MINIO_SECRET_KEY', 'lakehouse')

        # Check if aws_default connection exists
        conn = session.query(Connection).filter(Connection.conn_id == 'aws_default').first()

        if not conn:
            # Create aws_default connection for MinIO
            conn = Connection(
                conn_id='aws_default',
                conn_type='aws',
                login=minio_access_key,
                password=minio_secret_key,
                extra=json.dumps({'endpoint_url': minio_endpoint}),
            )
            session.add(conn)
            session.commit()
            print(f'Created aws_default connection for MinIO remote logging (endpoint: {minio_endpoint})')
        else:
            print(f'aws_default connection already exists')
        "

  # Ingress configuration
  ingress:
    enabled: false
    web:
      enabled: false
      annotations: {}
      hosts: []

  # Metrics and monitoring
  metrics:
    enabled: false
    serviceMonitor:
      enabled: false
      interval: 30s
      selector:
        prometheus: kube-prometheus

  # Service account
  serviceAccount:
    create: true
    name: ""
    annotations: {}

  # Security context
  securityContext:
    runAsUser: 50000
    fsGroup: 0

  # Pod security context
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 50000
    fsGroup: 0

  # Extra volumes
  extraVolumes: []

  # Extra volume mounts
  extraVolumeMounts: []

  # Triggerer (for deferrable operators)
  triggerer:
    enabled: false
    replicas: 1
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 200m

