# Airflow Helm Chart Values
# Default configuration for Apache Airflow workflow orchestrator
# Override with environment-specific values files

# Global settings
global:
  namespace: lakehouse-platform

# Airflow subchart configuration
# All values are passed to the upstream Airflow Helm chart
airflow:
  # Executor type: KubernetesExecutor for container-based jobs
  # Reference: contracts/workflow-orchestration.md section 5.1
  executor: "KubernetesExecutor"
  
  # Secrets
  fernetKeySecretName: airflow-fernet-key
  webserverSecretKeySecretName: airflow-webserver-secret

  # Image configuration
  images:
    airflow:
      repository: apache/airflow
      tag: "3.0.2-python3.11"
      pullPolicy: IfNotPresent

  # Use existing secret for admin user credentials
  # The secret should have keys: admin-user, admin-password, admin-email, admin-firstname, admin-lastname
  # Reference: platform/secrets/admin-password-sealed-secret.yaml
  # Generated by: scripts/generate-secrets.sh
  adminUserSecretName: "airflow-admin-password"

  # Webserver configuration
  webserver:
    replicas: 1

    defaultUser:
      enabled: false

    resources:
      requests:
        memory: 512Mi
        cpu: 250m
      limits:
        memory: 1Gi
        cpu: 500m

    service:
      type: ClusterIP

    # Probes configuration
    livenessProbe:
      initialDelaySeconds: 60
      timeoutSeconds: 30
      failureThreshold: 5
      periodSeconds: 20

    readinessProbe:
      initialDelaySeconds: 60
      timeoutSeconds: 30
      failureThreshold: 5
      periodSeconds: 20

    startupProbe:
      timeoutSeconds: 30
      failureThreshold: 60
      periodSeconds: 10

  # Scheduler configuration
  scheduler:
    replicas: 1
    
    resources:
      requests:
        memory: 512Mi
        cpu: 250m
      limits:
        memory: 1Gi
        cpu: 500m

  # PostgreSQL (metadata database)
  postgresql:
    enabled: true
    image:
      registry: docker.io
      repository: bitnami/postgresql
      tag: "latest"
    auth:
      existingSecret: postgres-creds
      database: airflow
    primary:
      extraEnvVars:
        - name: POSTGRESQL_USERNAME
          valueFrom:
            secretKeyRef:
              name: postgres-creds
              key: username
        - name: POSTGRESQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-creds
              key: password
      persistence:
        enabled: true
        size: 8Gi
      initdb:
        scripts:
          create-iceberg-db.sql: |
            CREATE DATABASE iceberg;
          create-sales-data-table.sql: |
            CREATE TABLE IF NOT EXISTS sales_data (
                id SERIAL PRIMARY KEY,
                product_name VARCHAR(100),
                quantity INTEGER,
                price DECIMAL(10,2),
                sale_date DATE
            );
            INSERT INTO sales_data (product_name, quantity, price, sale_date) VALUES
                ('Laptop', 2, 999.99, '2026-01-10'),
                ('Phone', 5, 599.99, '2026-01-11'),
                ('Headphones', 10, 79.99, '2026-01-12'),
                ('Keyboard', 3, 149.99, '2026-01-13'),
                ('Mouse', 8, 49.99, '2026-01-14');

  # Redis (disabled for KubernetesExecutor)
  redis:
    enabled: false

  # DAGs configuration
  dags:
    persistence:
      enabled: false
      # size: 1Gi
      # storageClassName: ""

    # GitSync configuration for DAG deployment
    # gitSync:
    #   enabled: true
    #   repo: https://github.com/SeoHyungjun/lakehouse-dags.git
    #   branch: main
    #   subPath: "dags"
    #   wait: 60
    #   maxFailures: 0
    #   credentialsSecret: git-sync-cred

  # Airflow configuration
  # Reference: contracts/workflow-orchestration.md section 12
  config:
    core:
      # dags_folder: "/opt/airflow/dags"
      load_examples: "False"
      executor: "KubernetesExecutor"
      parallelism: 32
      max_active_tasks_per_dag: 16
      max_active_runs_per_dag: 16
      dag_dir_list_interval: 30
      min_file_process_interval: 5
    
    webserver:
      expose_config: "True"
      rbac: "True"
      authenticate: "True"
      auth_backend: "airflow.api.auth.backend.basic_auth"
      enable_proxy_fix: "True"
    
    api:
      auth_backends: "airflow.api.auth.backend.basic_auth"
    
    kubernetes:
      namespace: lakehouse-platform
      worker_container_repository: apache/airflow
      worker_container_tag: "3.0.2-python3.11"
      delete_worker_pods: "True"
      delete_worker_pods_on_failure: "False"
      worker_pods_creation_batch_size: 1
      multi_namespace_mode: "False"
    
    logging:
      remote_logging: "False"
      colored_console_log: "True"
      logging_level: "INFO"
    
    metrics:
      statsd_on: "False"

  # Environment variables
  env:
    - name: AIRFLOW__CORE__EXECUTOR
      value: "KubernetesExecutor"
    - name: AIRFLOW__KUBERNETES__NAMESPACE
      value: "lakehouse-platform"

  # Use existing secret for metadata connection
  data:
    metadataSecretName: airflow-db-connection

  # Extra environment variables
  extraEnv: null

  # Extra environment variables from secrets
  # Note: extraEnvFrom expects a templated string, not an array
  extraEnvFrom: |
    - secretRef:
        name: airflow-connections
    - secretRef:
        name: minio-creds
    - secretRef:
        name: git-sync-cred

  # Database migration job configuration
  # Reference: https://github.com/apache/airflow/tree/main/chart
  # For ArgoCD/Flux/Rancher/Terraform, useHelmHooks and applyCustomEnv MUST be false
  migrateDatabaseJob:
    enabled: true
    useHelmHooks: false
    applyCustomEnv: false
    # Automatically run migrations on ArgoCD sync
    jobAnnotations:
      "argocd.argoproj.io/hook": Sync

  # Ingress configuration
  ingress:
    enabled: false
    web:
      enabled: false
      annotations: {}
      hosts: []

  # Metrics and monitoring
  metrics:
    enabled: false
    serviceMonitor:
      enabled: false
      interval: 30s
      selector:
        prometheus: kube-prometheus

  # Service account
  serviceAccount:
    create: true
    name: ""
    annotations: {}

  # Security context
  securityContext:
    runAsUser: 50000
    fsGroup: 0

  # Pod security context
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 50000
    fsGroup: 0

  # Extra volumes
  extraVolumes: []

  # Extra volume mounts
  extraVolumeMounts: []

  # Triggerer (for deferrable operators)
  triggerer:
    enabled: false
    replicas: 1
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 200m

